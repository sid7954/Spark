PART-3 README

-Install Apache Hadoop and Apace Spark and esure that they are running.

- Run the run.sh script from this directory [Our run.sh assumes that Spark is added to path and hence we directly use the spark-submit command]

bash run.sh <Master Spark URL with port> <Spark Local Directory specified as spark.local.dir> <HDFS source for csv file> <HDFS location of output folder> <Number of Partitions>


<Master Spark URL with port>  
Example : spark://128.104.222.133:7077

<Spark Local Directory specified as spark.local.dir>   
Example : "spark.local.dir=/data/spark"

<HDFS source for csv file> 
Example : hdfs://128.104.222.133:9000/exports.csv 

<HDFS location of output folder> 
Example : hdfs://128.104.222.133:9000/part2     
[The final file will be named part-00000 inside the folder name provided as this argument]

<Number of Partitions>
Example : 60    [When not specified, use 60 as the execution will occur fastest]


An example run of run.sh is as follows:

bash run.sh spark://128.104.222.133:7077 "spark.local.dir=/data/spark" hdfs://128.104.222.133:9000/enwiki_articles hdfs://128.104.222.133:9000/part3 60
